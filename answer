1) What is the definition of Hive? What is the present version of Hive?
answer :- hive is dataware house which will run on top of hadoop.
present version is hive 3.1.3.

2) Is Hive suitable to be used for OLTP systems? Why?
answer:-- No Hive does not provide insert and update at row level. So it is not suitable for OLTP system.

3) How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.
answer:-- hive is only of used for maintain data warehouse.
it will run on HQL .Hive supports ACID transactions on tables that store ORC file format.

4)Explain the hive architecture and the different components of a Hive architecture?
hive architecture.
-----------------

1)multiple clients like thrift app, jdbc app, odbc app.

2)client will gives data to driver .
3)driver to compiler 
4)driver to metastore
5)driver to optimizer.
6) the driver output goes to mapreduce and yarn part.
7)map reduces, yarn  (resource management and processing)
8)mapreduce to hdfs part file receives
9)last is hdfs this is for storage of file

5)Mention what Hive query processor does? And Mention what are the components of a Hive query processor?
answer:-- Hive query processor convert graph of MapReduce jobs with the execution time framework.
Parse and Semantic Analysis (ql/parse)
Metadata Layer (ql/metadata)
Type Interfaces (ql/typeinfo)
Sessions (ql/session)
Map/Reduce Execution Engine (ql/exec)
Plan Components (ql/plan)
Hive Function Framework (ql/udf)
Tools (ql/tools).

6) what are the 3 modes in which we can operate hive?
answer:-- Standalone Mode.
Pseudo-distributed Mode.
Fully-Distributed Mode.

7) What are the features and limitations of hive?
 answer:--
Limitations of Hive
Hive doesn't support OLTP. Hive supports Online Analytical Processing (OLAP), but not Online Transaction Processing (OLTP).
It doesn't support subqueries.
It has a high latency.
Hive's Features
Hive is designed for querying and managing only structured data stored in tables.
Hive is scalable, fast, and uses familiar concepts.
Schema gets stored in a database, while processed data goes into a Hadoop Distributed File System (HDFS)

8) How to create a Database in HIVE?
answer:-- create database name.

9) How to create a table in HIVE?
answer:-- create table name( id, name etc).
row format delimited .
fields terminated by some limitation;

10) What do you mean by describe and describe extended and describe
formatted with respect to database and table.
answer:--
describe extended - This will show table columns, data types, and other details of the table. 
Other details will be displayed in single line.
describe formatted - This will show table columns, data types, and other details of the table.
Other details will be displayed into multiple lines

11)How to skip header rows from a table in Hive?
answer:-- by using tabular properties like 
tbl properties("skip.header.line.count"= "1")

12)What is a hive operator? What are the different types of hive operators?
answer:-- Hive operators are used for mathematical operations on operands. It returns specific value as per the logic applied.
There are four types of operators in Hive:
Relational Operators.
Arithmetic Operators.
Logical Operators.
Complex Operators.

13) 13.Explain about the Hive Built-In Functions
answer:-- The Hive Built-in functions are categorized as Mathematical function, Collection function, Type conversion function, Date function, Conditional function, and String function. 

14)write hive DDL and DML commands.
The various Hive DML commands are:
LOAD.
SELECT.
INSERT.
DELETE.
UPDATE.

The several types of Hive DDL commands are:
CREATE.
SHOW.
DESCRIBE.
USE.
DROP.
ALTER.
EXPORT.
IMPORT.
TRUNCATE.

15)15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and
CLUSTER BY in Hive.
answer:-- sort by:
That means the files are sorted per reducer basis only, and not globally. 
Note that the final output data is not globally sorted and may have overlapping data ranges.

order by :
ORDER BY clause orders the data globally.
Because it ensures the global ordering of the data, all the data need to be passed from a single reducer only.

distribute by:
DISTRIBUTE BY clause is used to distribute the input rows among reducers. 
It ensures that all rows for the same key columns are going to the same reducer. 
the DISTRIBUTE BY clause may output N number of unsorted files where N is the number of reducers used in the query processing.
But, the output files do not contain overlapping data ranges.

clusterby:
CLUSTER BY clause is a combination of DISTRIBUTE BY and SORT BY clauses together.
That means the output of the CLUSTER BY clause is equivalent to the output of DISTRIBUTE BY + SORT BY clauses. 
As a result, the order by clause outputs one single file only. 

16) Difference between "Internal Table" and "External Table" and Mention
when to choose “Internal Table” and “External Table” in Hive?
answer:--internal table 
   if we table metadata will be stored in meta store 
	data of the table will be stored in warehouse directory inside hdfs configured for hive.
	if we drop a internal table then its metadata & original data both,will be removed
2)external table:---
  meta data will be stored in metastore.
	raw data will not be stored in hive warehouse directory but it will stored in hdfs location .
	it is like external reference to the raw data.
	if we remove the table then only metadata will be deleted but original data will be untouched .
  
  We create an external table for external use as when we want to use the data outside the Hive.
  External tables are stored outside the warehouse directory. They can access data stored in sources such as remote HDFS locations or Azure Storage Volumes
  
  internal hive 
  Use INTERNAL tables when: The data is temporary. You want Hive to completely manage the lifecycle of the table and data.
  
  17) Where does the data of a Hive table get stored?
  answer:-- Hive stores its database and table metadata in a metastore,
  
  18) Is it possible to change the default location of a managed table?
  answer:- yes  we can change the location of managed table in hive by giving location path .
  
  19) What is a metastore in Hive? What is the default database provided by
Apache Hive for metastore?

answer:-- Metastore is the central repository of Apache Hive metadata. It stores metadata for Hive tables 
Derby is the default database for the embedded metastore.

20) Why does Hive not store metadata information in HDFS?
answer:-- Hive stores metadata information in the metastore using RDBMS instead of HDFS.
21).What is a partition in Hive? And Why do we perform partitioning in
Hive?
answer:--
partition in hive :--- it is away of dividing table into related parts based on the values of aparticular column.
 the data is stored in slices, the query response time becomes faster.
 
 22) What is the difference between dynamic partitioning and static partitioning?
answer:-- dynamic partition
**  here column values will be unknown to us 
** hive will identify the unique value from partitioned column.
**it takes longer time while loading the data.
static partition:-
** here value of partition column will be known to us .
** we load the data in a specific partition .
** static partition takes less time. while loading data

23) How do you check if a particular partition exists?
answer:-- SHOW PARTITIONS table_name 
PARTITION(partitioned_column=’partition_value’)

24) How can you stop a partition form being queried?
answer:--By using the ENABLE OFFLINE clause with ALTER TABLE atatement.

25)  what is meant by bucketing ?.Why do we need buckets? How Hive distributes the rows into buckets?
answer:--bucketing offers the additional functionality of dividing large datasets into smaller and more manageable sets called buckets.
Bucketing in hive is useful when dealing with large datasets 
that may need to be segregated into clusters for more efficient management and to be able to perform join queries with other large datasets
based on the hashing technique.

26) In Hive, how can you enable buckets?
answer:-- set.hive.enforce.bucketing=true;

27) How does bucketing help in the faster execution of queries?
answer:-- Bucketing improves performance by shuffling and sorting data prior to downstream operations such as table joins.

28)  How to optimise Hive Performance? Explain in very detail.
answer :-- 
First, tweak your data through partitioning, bucketing, compression, etc.
Improving the execution of a hive query is another Hive query optimization technique.
You can do this by using Tez, avoiding skew, and increasing parallel execution.

29) What is the use of Hcatalog?
answer:- HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications.

30) Explain about the different types of join in Hive.
answer:-- there are several types of Hive join –
Hive inner join, hive left outer join, hive right outer join, and hive full outer join.

31) Is it possible to create a Cartesian join between 2 tables, using Hive?
 answer:-- Cross join, also known as Cartesian product, 
 is a way of joining multiple tables in which all the rows or tuples from one table are paired with the rows and tuples from another table.
 
 32) Explain the SMB Join in Hive?
 answer:-- SMB is a join performed on bucket tables that have the same sorted, bucket, and join condition columns.
 It reads data from both bucket tables and performs common joins (map and reduce triggered) on the bucket tables.
 
 33) What is the difference between order by and sort by which one we should use?
 answer:-- The difference between "order by" and "sort by" is that the former guarantees 
 total order in the output while the latter only guarantees ordering of the rows within a reducer.
 If there are more than one reducer, "sort by" may give partially ordered final results
 
 34) What is the usefulness of the DISTRIBUTED BY clause in Hive?
 answer:-- DISTRIBUTE BY clause is used to distribute the input rows among reducers.
 It ensures that all rows for the same key columns are going to the same reducer.
 So, if we need to partition the data on some key column, we can use the DISTRIBUTE BY clause in the hive queries.
 
 35) How does data transfer happen from HDFS to Hive?
 answer:-- You create a single Sqoop import command that imports data from diverse data sources, such as a relational database, into HDFS.
 Convert the data to ORC format. To query data in HDFS in Hive, you apply a schema to the data and then store data in ORC format.
 Incrementally update the imported data

36)Wherever (Different Directory) I run the hive query, it creates a new
metastore_db, please explain the reason for it?
answer:- bascially it creates the local metastore, while we run the hive in embedded mode.
Also, it looks whether metastore already exist or not before creating the metastore.

37) What will happen in case you have not issued the command: ‘SET
hive.enforce.bucketing=true;’ before bucketing a table in Hive?

answer:-- The command: ‘SET hive.enforce.bucketing=true;’ allows you to have the correct number of reducer while using ‘CLUSTER BY’ clause for bucketing a column. 
In case it’s not done, one may find the number of files generated in the table directory to be unequal to the number of buckets.
As an alternative solution, one may also set the number of reducer equal to the number of buckets by using set mapred.reduce.task = num_bucket. 

38) Can a table be renamed in Hive?
answer:-- by using alter command.
command like alter table table_name RENAME to new_table_name;

39) Write a query to insert a new column(new_col INT) into a hive table at a
position before an existing column (x_col)
answer:-- ALTER TABLE h_table
add COLUMN new_col INT


40) What is serde operation in HIVE?
answer:-- Hive allows the framework to read or write data in a particular format.
SerDe is short for Serializer/Deserializer. Hive uses the SerDe interface for IO. 
The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing.

41)Explain how Hive Deserializes and serialises the data?
answer:-- when querying a table, a SerDe will deserialize a row of data from the bytes in the file to objects used internally by Hive to operate on that row of data. 
when performing an INSERT or CTAS (see “Importing Data” on page 441),
the table’s SerDe will serialize Hive’s internal representation of a row of data into the bytes that are written to the output file

42) .Write the name of the built-in serde in hive.
answer:-- Basically, to read and write HDFS files Hive uses these FileFormat classes currently:

TextInputFormat/HiveIgnoreKeyTextOutputFormat
It read/write data in plain text file format.

SequenceFileInputFormat/SequenceFileOutputFormat
It read/write data in Hadoop SequenceFile format.
Moreover, to serialize and deserialize data Hive uses these Hive SerDe classes currently:

MetadataTypedColumnsetSerDe
So, to read/write delimited records we use this Hive SerDe. Such as CSV, tab-separated control-A separated records (sorry, quote is not supported yet).

LazySimpleSerDe
Also, to read the same data format as MetadataTypedColumnsetSerDe and TCTLSeparatedProtocol, we can use this Hive SerDe. Moreover, it creates Objects in a lazy way. 
Hence, that offers better performance.

43) What is the need for custom SerDe?
answer:-- 
A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format. 
Anyone can write their own SerDe for their own data formats. 
See Hive SerDe for an introduction to SerDes.

44) Can you write the name of a complex data type(collection data types) in Hive?
answer:-- Array, Map, Struct and union.

45) Can hive queries be executed from script files? How?
answer:--- It is possible by using the source command.
example:-- Hive> source /path/to/file/file_with_query.hql

46)What are the default record and field delimiter used for hive text files?
answer:-- The default record delimiter is − \n And the filed delimiters are − \001,\002,\003 

47) How do you list all databases in Hive whose name starts with s?
answer:-- hive>show databases like '%s';

48) What is the difference between LIKE and RLIKE operators in Hive?
answer:-- LIKE is an operator similar to LIKE in SQL. We use LIKE to search for string with similar text.
          E.g. user_name LIKE ‘%Smith’

RLIKE (Right-Like) is a special function in Hive where if any substring of A matches with B then it evaluates to true. 
It also obeys Java regular expression pattern. Users don't need to put % symbol for a simple match in RLIKE.

Hive provides RLIKE operator that can be used for searching advanced Regular Expressions in Java.

E.g. user_name RLIKE ‘.(Smith|Sam).’

49) How to change the column data type in Hive?
answer:-- ALTER TABLE table_name CHANGE column_name column_name new_datatype;

50) How will you convert the string ’51.2’ to a float value in the particular column?
answer:--- select cast(1 as float);

51) What will be the result when you cast ‘abc’ (string) as INT?
answer:-- hive return  null 

52) What does the following query do?
 INSERT OVERWRITE TABLE employees
 PARTITION (country, state)
 SELECT ..., se.cnty, se.st
 FROM staged_employees se;
 
 answer: It creates partition on table employees with partition values coming from the columns in the select clause.
 It is called Dynamic partition insert.
 
 53)  Write a query where you can overwrite data in a new table from the existing table.
answer :-- insert overwrite table new_table_name select * from existing_table ;

54) What is the maximum size of a string data type supported by Hive?
Explain how Hive supports binary formats.
answer:--  By default, the columns metadata for Hive does not specify a maximum data length for STRING columns.
The driver has the parameter DefaultStringColumnLength, default is 255 maximum value.
Does Hive support binary data type?
Hive supports two miscellaneous data types: Boolean and Binary : Boolean accepts true or false values. Binary is a sequence of bytes. 
It is similar to the VARBINARY data type found in many relational databases.

55) What File Formats and Applications Does Hive Support?

answer:-- file format are orc file.
parquet file format,
avero format,
rc format,
sequence format,
text format

hive support for orc file format.

56) How do ORC format tables help Hive to enhance its performance?
answer:-- Using the ORC format leads to a reduction in the size of the data stored, as this file format has high compression ratios.
As the data size is reduced, the time to read and write the data is also reduced.
The ORC format improves query performance also by the way it stores data in a file.

57) How can Hive avoid mapreduce while processing the query?
answer:-- You can make Hive avoid MapReduce to return query results by setting the hive. exec. mode. local.

58)What is view and indexing in hive?
answer:--  A view is just a way of abbreviating a subquery.
An index is used to optimize matching column data.

59)Can the name of a view be the same as the name of a hive table?
answer:-- The name of a view must be unique, and it cannot be the same as any table or database or view's name.

60) What types of costs are associated in creating indexes on hive tables?
answer:--- Basically, there is a processing cost in arranging the values of the column on which index is created since Indexes occupies.

61) Give the command to see the indexes on a table.
answer:-- SHOW INDEX FROM yourtable;

62) Explain the process to access subdirectories recursively in Hive queries.
answer:-- hive> Set mapred. input. dir.

63) If you run a select * query in Hive, why doesn't it run MapReduce?
answer:--- Hive requires a map-reduce job since it needs to extract the 'column' from each row by parsing it from the file it loads.

64) What are the uses of Hive Explode?
answer:-- Explode in Hive is used to convert complex data types into desired table formats. explode
UDTF basically emits all the elements in an array into multiple rows.

65) What is the available mechanism for connecting applications when we run Hive as a server?
answer:-- Thrift Client:

66) Can the default location of a managed table be changed in Hive?
answer:-- Yes, you can do it by using the clause – LOCATION '<hdfs_path>'

67) What is the Hive ObjectInspector function?
answer:-- Hive ObjectInspector is a group of flexible APIs to inspect value in different data representation, and developers can extend those API as needed, so technically, object inspector supports arbitrary data type in java. 
Fortunately, only few built-in Hive Object Inspectors are used in generic udf/udaf/udtf evaluation.

68) What is UDF in Hive?
answer:-- The Db2 Big SQL environment in Hadoop includes the Hive user-defined functions package. 
This set of functions is an optional package that you can install to use some of the Hive open source user-defined functions in your Db2 Big SQL queries.

69) Write a query to extract data from hdfs to hive.
answer:-- load data inpath 'location_name' into table table_name;

70) What is TextInputFormat and SequenceFileInputFormat in hive.
answer:--SequenceFile is a flat file consisting of binary key/value pairs. It is extensively used in MapReduce as input/output formats. 
It is also worth noting that, internally, the temporary outputs of maps are stored using SequenceFile.
 
TextInputFormat is one of the file formats of Hadoop. As the name suggest,it is used to read lines of text files.
Basically it helps in generating key-value pairs from the text.

71)How can you prevent a large job from running for a long time in a hive?
answer:-- setting the MapReduce jobs to execute in strict mode set hive.

72) When do we use explode in Hive?
answer:-- The explode function explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array.

73) Can Hive process any type of data formats? Why? Explain in very detail
answer:-- Hive supports four file formats those are TEXTFILE, SEQUENCEFILE, ORC and RCFILE (Record Columnar File).
For single user metadata storage, Hive uses derby database and for multiple user Metadata or shared Metadata case Hive uses MYSQL

74)Whenever we run a Hive query, a new metastore_db is created. Why?
 answer:-- 
 A local metastore is created when we run Hive in an embedded mode. 
Before creating, it checks whether the metastore exists or not, and this metastore property is defined in the configuration file, hive-site.xml. 
The property is:

javax.jdo.option.ConnectionURL
with the default value:

jdbc:derby:;databaseName=metastore_db;create=true
Therefore, we have to change the behavior of the location to an absolute path so that from that location the metastore can be used.

75)Can we change the data type of a column in a Hive table? Write a complete query.
answer:-- 
Use the ALTER command on the table with the CHANGE clause.
alter table table_name change column_name new_column_name datatype;

76)While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file?
answer while loading data in hive from hdfs system 
the path says that data is in hdfs 
command will be load data inpath '/directoty/file_name' into table table_name;

77) What is the precedence order in Hive configuration?
answer:-- In Hive we can use following precedence order to set the configurable properties.

Hive SET command has the highest priority
-hiveconf option from Hive Command Line
hive-site.xml file
hive-default.xml file
hadoop-site.xml file
hadoop-default.xml file

78) Which interface is used for accessing the Hive metastore?
answer:--  WebHCat API web interface can be used for Hive commands.
It is a REST API that allows applications to make HTTP requests to access the Hive metastore (HCatalog DDL). 
It also enables users to create and queue Hive queries and commands.
 
 79) Is it possible to compress json in the Hive external table
answer:-- yes we can compress json in hive by using some of compression properties
set mapred.output.compress=true;
set hive.exec.compress.output=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;

80)What is the difference between local and remote metastores?
answer:--
Local Metastore:- Here metastore service still runs in the same JVM as Hive but it connects to a database running in a separate process either on same machine or on a remote machine. 
Remote Metastore:- Metastore runs in its own separate JVM not on hive service JVM.

81) What is the purpose of archiving tables in Hive?
answer:-- You can use Hadoop archiving to reduce the number of hdfs files in the Hive table partition.

82) What is DBPROPERTY in Hive?
answer:-- DBPROPERTIES takes multiple arguments in the form of a key-value pair.

83)Differentiate between local mode and MapReduce mode in Hive.
answer:--  local mode;
as the main HiveServer process, the Hive metastore service runs in the same process, but make sure Hive metastore database runs in a separate process, as well as it can be on a separate host,
 
MapReduce: It is a parallel programming model for processing large amounts of structured, semi-structured, and unstructured data on large clusters of commodity hardware.
HDFS:Hadoop Distributed File System is a part of Hadoop framework, used to store and process the datasets.



